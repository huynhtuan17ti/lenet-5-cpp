{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC14120 â€“ PARALLEL PROGRAMMING - Final project report\n",
    "\n",
    "### Authors\n",
    "Student 1: Huynh Minh Tuan - 20120024  \n",
    "Student 2: Huynh Minh Tu - 20120393\n",
    "\n",
    "### Assignment\n",
    "Huynh Minh Tu:\n",
    "- Setup and re-organize the dnn project using bazel instead of cmake.\n",
    "- Implement basic parallel version of convolutional layer.\n",
    "- Implement tiled shared memory parallel version of convolutional layer.\n",
    "- Train modified LeNet-5 model, setup inference and testset evaluate.\n",
    "- Write report.\n",
    "\n",
    "Huynh Minh Tuan:\n",
    "- Setup third party Eigen in bazel.\n",
    "- Setup cuda compile in bazel.\n",
    "- Upgrade and support implementing tiled shared memory parallel version of convolutional layer.\n",
    "- Implement batch samples parallel version of convolutional layer.\n",
    "- Setup run configs for inference and report.\n",
    "- Write report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "In this project, we setup the whole code with [bazel](https://bazel.build/) instead of `CMake`. `Bazel` provides simple code compile setup and supports cuda as well.\n",
    "\n",
    "- Check [here](https://bazel.build/install) for how to install bazel in each OS.\n",
    "- Check [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) for installing cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 25 14:45:49 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    Off | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   45C    P8               2W /  60W |     59MiB /  8188MiB |     20%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2573      G   /usr/lib/xorg/Xorg                           45MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bazel 6.4.0\n"
     ]
    }
   ],
   "source": [
    "!bazel --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "#### Problem statement\n",
    "Implementing and optimizing the forward-pass of convolutional layers in modified LeNet-5 using C++ and CUDA.\n",
    "\n",
    "An example of a process of CNN model.  \n",
    "\n",
    "\n",
    "<img src=\"images/cnn_illustration.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "In the scope of this project, given [MNIST fashion dataset](https://github.com/zalandoresearch/fashion-mnist), we need to implement and train a modified LeNet-5 model that is able to predict the fashion type of an input image using C++.\n",
    "\n",
    "\n",
    "#### How can GPU help to speed up the process\n",
    "Inside a computer vision network, processing matrices is essential. In sequential way, matrix processes each pixel step by step.\n",
    "\n",
    "An observation shows that each pixel inside a matrix can be handled independently. Accordingly, we can utilize the power of GPU to force the network process multiple pixels in a single matrix simultaneously.\n",
    "\n",
    "In this project, we focus on using cuda in optimizing the speed performance of convolutional layers in modified LeNet-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer in CNN\n",
    "Firstly, to get more insights about how our optimization works. Let's take a look into convolutional layer in CNN. \n",
    "\n",
    "The figure below shows the process of a convolutional layer.\n",
    "\n",
    "- A single input matrix would be `channel_in * width_in * height_in`.  \n",
    "- A kernel matrix has the shape `channel_in * kernel_width * kernel_height * channel_out`.  \n",
    "- A single output matrix results `channel_out * width_out * height_out`.\n",
    "\n",
    "<img src=\"images/conv.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "#### 1. Sequential implementation\n",
    "\n",
    "In sequential implementation, the given code of convolutionl layers does following these steps:\n",
    "- Kernel is represented as a vector.\n",
    "- Iterating each pixel in output matrix.\n",
    "- For each pixel, find the corresponding pixels in input matrix and arrange the pixels' order following the kernel as a vector.\n",
    "-   .\n",
    "\n",
    "#### 2. Parallel implementation\n",
    "\n",
    "We discuss 2 parts that are able to apply parallel process.\n",
    "- Each pixel in output matrix can be handled independently.\n",
    "- In CNN, for boosting the performance, usually model handles multiple samples at once. As a result, we decide to follow the idea: process a batch sample at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel optimization\n",
    "#### 1. Illustration\n",
    "Below is the workflow of our final optimaztion (the fastest version).\n",
    "\n",
    "The workflow would be:\n",
    "- A `blockSize` is defined. `blockSize` has 3 dims, we ultilize all of them.\n",
    "- The first and second dims (`blockSize.x`, `blockSize.y`) to parallelize each single input matrix. For each output tile, we get the necessary input tile and copy to SMEM, and apply kernel filter on SMEM instead of GMEM.\n",
    "- Using the same idea of processing batch size in most of AI frameworks (torch, tensorflow). The third dim (`blockSize.z`) is used to handle `n_samples` data, make them process simultaneously. By using `cudaEvent`, dividing batch data into streams, we are able to make the performance better.\n",
    "\n",
    "<img src=\"images/optimize_conv.png\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "#### 2. Versions\n",
    "In order to show the impacts of optimzations clearly, we have implemented 3 versions for the optimization.\n",
    "- Version 1: Simple conv implementation, utilizing parallel processing in cuda.\n",
    "- Version 2: Using tiled shared memory convolution for each input matrix.\n",
    "- Version 3: Upgrade from version 2, adding cuda streams to handle a batch data sample simultaneously.\n",
    "\n",
    "The figure above illustrates the workflow of version 3, which is the optimal and fastest version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "- To ensure that our conv implementation returns correct value, we have trained model with the given data MNIST Fashion. The accuracy on the test data is around 0.82.\n",
    "- For benchmarking, we evaluate each version of cuda conv on the test dataset. Note that we only need to focus the elapsed time of layer 1 and 4, which are convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:23:13) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:23:13) \u001b[32mINFO: \u001b[0mBuild option --cxxopt has changed, discarding analysis cache.\n",
      "(15:23:13) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 2367 targets configured).\n",
      "(15:23:13) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:23:14) \u001b[32mINFO: \u001b[0mElapsed time: 1.813s, Critical Path: 1.70s\n",
      "(15:23:14) \u001b[32mINFO: \u001b[0m15 processes: 1 internal, 14 local.\n",
      "(15:23:14) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |             13690 |\n",
      "| Layer 2  |               178 |\n",
      "| Layer 3  |              4288 |\n",
      "| Layer 4  |             10399 |\n",
      "| Layer 5  |                52 |\n",
      "| Layer 6  |              1308 |\n",
      "| Layer 7  |               613 |\n",
      "| Layer 8  |                 5 |\n",
      "| Layer 9  |               216 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                21 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 0 (host version)\n",
    "!bazel run --noshow_progress //:inference --config=report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:22:32) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 0 targets configured).\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mElapsed time: 0.053s, Critical Path: 0.00s\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0m1 process: 1 internal.\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |               519 |\n",
      "| Layer 2  |               182 |\n",
      "| Layer 3  |              4324 |\n",
      "| Layer 4  |              1398 |\n",
      "| Layer 5  |                54 |\n",
      "| Layer 6  |              1315 |\n",
      "| Layer 7  |               611 |\n",
      "| Layer 8  |                 6 |\n",
      "| Layer 9  |               216 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                21 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 1\n",
    "!bazel run --noshow_progress //:inference --config=cuda --config=report --//:conv_ver=v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:22:44) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:22:44) \u001b[32mINFO: \u001b[0mBuild option --//:conv_ver has changed, discarding analysis cache.\n",
      "(15:22:44) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 2367 targets configured).\n",
      "(15:22:44) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:22:46) \u001b[32mINFO: \u001b[0mElapsed time: 1.837s, Critical Path: 1.71s\n",
      "(15:22:46) \u001b[32mINFO: \u001b[0m11 processes: 1 internal, 10 local.\n",
      "(15:22:46) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |               396 |\n",
      "| Layer 2  |               183 |\n",
      "| Layer 3  |              4327 |\n",
      "| Layer 4  |              1072 |\n",
      "| Layer 5  |                54 |\n",
      "| Layer 6  |              1313 |\n",
      "| Layer 7  |               612 |\n",
      "| Layer 8  |                 6 |\n",
      "| Layer 9  |               215 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                22 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 2\n",
    "!bazel run --noshow_progress //:inference --config=cuda  --config=report --//:conv_ver=v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:22:57) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:22:57) \u001b[32mINFO: \u001b[0mBuild option --//:conv_ver has changed, discarding analysis cache.\n",
      "(15:22:57) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 2367 targets configured).\n",
      "(15:22:57) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:22:59) \u001b[32mINFO: \u001b[0mElapsed time: 1.746s, Critical Path: 1.62s\n",
      "(15:22:59) \u001b[32mINFO: \u001b[0m11 processes: 1 internal, 10 local.\n",
      "(15:22:59) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |                56 |\n",
      "| Layer 2  |               179 |\n",
      "| Layer 3  |              4286 |\n",
      "| Layer 4  |                28 |\n",
      "| Layer 5  |                52 |\n",
      "| Layer 6  |              1305 |\n",
      "| Layer 7  |               613 |\n",
      "| Layer 8  |                 6 |\n",
      "| Layer 9  |               216 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                21 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 3\n",
    "!bazel run --noshow_progress //:inference --config=cuda --config=report --//:conv_ver=v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All estimated time was calculated in miliseconds.\n",
    "\n",
    "| Version       | Layer 1          | Layer 4        |\n",
    "| ------------- | ---------------- | -------------- |\n",
    "| 0 (host)      | 13690            | 10399          |\n",
    "| 1             | 519              | 1398           |\n",
    "| 2             | 396              | 1072           |\n",
    "| 3             | 56               | 28             |\n",
    "\n",
    "\n",
    "From the result, can see that the version 3 has a significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "#### Each member\n",
    "- Huynh Minh Tu:\n",
    "    - Difficulties:\n",
    "        - Setup and re-organize the whole project with bazel instead of cmake.\n",
    "        - The way that matrix in Eigen works and allocates in memory gives a lot of difficuties to implement a cuda version.\n",
    "    - Learns:\n",
    "        - Compiling C++ using Bazel tool.\n",
    "        - Making an cuda-based C++ object in a project.\n",
    "\n",
    "- Huynh Minh Tuan:\n",
    "    - Difficulties:\n",
    "        - Setup third-party Eigen in bazel.\n",
    "    - Learns:\n",
    "        - Cuda setup with bazel.\n",
    "        - Implement batch data samples with C++ and cuda.\n",
    "\n",
    "#### Further plans\n",
    "- We haven't tried using atomic add in channel dim of matrix because the `blockSize` limits at 3 dims. But we believe that perhaps there is a way to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube video\n",
    "\n",
    "[video link](https://youtu.be/MWuiGVVIjVw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
