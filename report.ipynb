{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC14120 â€“ PARALLEL PROGRAMMING - Final project report\n",
    "\n",
    "### Authors\n",
    "Student 1: Huynh Minh Tuan - 20120024  \n",
    "Student 2: Huynh Minh Tu - 2012  \n",
    "\n",
    "### Problem statement\n",
    "Implementing and optimizing the forward-pass of convolutional layers in modified LeNet-5 using CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 25 14:45:49 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    Off | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   45C    P8               2W /  60W |     59MiB /  8188MiB |     20%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2573      G   /usr/lib/xorg/Xorg                           45MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer in CNN\n",
    "The figure below shows the process of a convolutional layer.\n",
    "\n",
    "<img src=\"images/conv.png\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer optimization using CUDA\n",
    "#### 1. Architecture\n",
    "<img src=\"images/optimize_conv.png\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "#### 2. Versions\n",
    "In the codebase, we have 3 versions for the optimization\n",
    "- Version 1: Simple conv implementation, ultilizing parallel processing in cuda.\n",
    "- Version 2: Using tiled shared memory convolution.\n",
    "- Version 3: Using cuda streams to handle a batch data sample simultaneously.\n",
    "\n",
    "The figure above illustrates the workflow of version 3, which is the optimal and fastest version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- To make sure that our conv implementation returns correct value, we have trained model with the given data MNIST Fashion. The accuracy on the test data is around 0.82.\n",
    "- For benchmarking, we evaluate each version of cuda conv on the test dataset. Note that we only need to focus the elapsed time of layer 1 and 4, which are convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:23:13) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:23:13) \u001b[32mINFO: \u001b[0mBuild option --cxxopt has changed, discarding analysis cache.\n",
      "(15:23:13) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 2367 targets configured).\n",
      "(15:23:13) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:23:14) \u001b[32mINFO: \u001b[0mElapsed time: 1.813s, Critical Path: 1.70s\n",
      "(15:23:14) \u001b[32mINFO: \u001b[0m15 processes: 1 internal, 14 local.\n",
      "(15:23:14) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |             13690 |\n",
      "| Layer 2  |               178 |\n",
      "| Layer 3  |              4288 |\n",
      "| Layer 4  |             10399 |\n",
      "| Layer 5  |                52 |\n",
      "| Layer 6  |              1308 |\n",
      "| Layer 7  |               613 |\n",
      "| Layer 8  |                 5 |\n",
      "| Layer 9  |               216 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                21 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 0 (host version)\n",
    "!bazel run --noshow_progress //:inference --config=report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:22:32) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 0 targets configured).\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mElapsed time: 0.053s, Critical Path: 0.00s\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0m1 process: 1 internal.\n",
      "(15:22:32) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |               519 |\n",
      "| Layer 2  |               182 |\n",
      "| Layer 3  |              4324 |\n",
      "| Layer 4  |              1398 |\n",
      "| Layer 5  |                54 |\n",
      "| Layer 6  |              1315 |\n",
      "| Layer 7  |               611 |\n",
      "| Layer 8  |                 6 |\n",
      "| Layer 9  |               216 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                21 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 1\n",
    "!bazel run --noshow_progress //:inference --config=cuda --config=report --//:conv_ver=v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:22:44) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:22:44) \u001b[32mINFO: \u001b[0mBuild option --//:conv_ver has changed, discarding analysis cache.\n",
      "(15:22:44) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 2367 targets configured).\n",
      "(15:22:44) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:22:46) \u001b[32mINFO: \u001b[0mElapsed time: 1.837s, Critical Path: 1.71s\n",
      "(15:22:46) \u001b[32mINFO: \u001b[0m11 processes: 1 internal, 10 local.\n",
      "(15:22:46) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |               396 |\n",
      "| Layer 2  |               183 |\n",
      "| Layer 3  |              4327 |\n",
      "| Layer 4  |              1072 |\n",
      "| Layer 5  |                54 |\n",
      "| Layer 6  |              1313 |\n",
      "| Layer 7  |               612 |\n",
      "| Layer 8  |                 6 |\n",
      "| Layer 9  |               215 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                22 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 2\n",
    "!bazel run --noshow_progress //:inference --config=cuda  --config=report --//:conv_ver=v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15:22:57) \u001b[32mINFO: \u001b[0mCurrent date is 2023-12-25\n",
      "(15:22:57) \u001b[32mINFO: \u001b[0mBuild option --//:conv_ver has changed, discarding analysis cache.\n",
      "(15:22:57) \u001b[32mINFO: \u001b[0mAnalyzed target //:inference (0 packages loaded, 2367 targets configured).\n",
      "(15:22:57) \u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "Target //:inference up-to-date:\n",
      "  bazel-bin/inference\n",
      "(15:22:59) \u001b[32mINFO: \u001b[0mElapsed time: 1.746s, Critical Path: 1.62s\n",
      "(15:22:59) \u001b[32mINFO: \u001b[0m11 processes: 1 internal, 10 local.\n",
      "(15:22:59) \u001b[32mINFO: \u001b[0mRunning command line: bazel-bin/inference\n",
      "\u001b[0mObject loaded from binary file: weights/lenet5_mnist_weight\n",
      "--------------------------------\n",
      "|  Network | Elapsed Time (ms) |\n",
      "--------------------------------\n",
      "| Layer 1  |                56 |\n",
      "| Layer 2  |               179 |\n",
      "| Layer 3  |              4286 |\n",
      "| Layer 4  |                28 |\n",
      "| Layer 5  |                52 |\n",
      "| Layer 6  |              1305 |\n",
      "| Layer 7  |               613 |\n",
      "| Layer 8  |                 6 |\n",
      "| Layer 9  |               216 |\n",
      "| Layer 10 |                 3 |\n",
      "| Layer 11 |                21 |\n",
      "| Layer 12 |                10 |\n",
      "--------------------------------\n",
      "Test acc = 0.8297\n"
     ]
    }
   ],
   "source": [
    "### Version 3\n",
    "!bazel run --noshow_progress //:inference --config=cuda --config=report --//:conv_ver=v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All estimated time was calculated in miliseconds.\n",
    "\n",
    "| Version       | Layer 1          | Layer 4        |\n",
    "| ------------- | ---------------- | -------------- |\n",
    "| 0 (host)      | 13690            | 10399          |\n",
    "| 1             | 519              | 1398           |\n",
    "| 2             | 396              | 1072           |\n",
    "| 3             | 56               | 28             | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
